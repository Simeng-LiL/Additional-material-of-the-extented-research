{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow2oM_NYc8bo",
        "outputId": "21d164fb-edc6-4709-e200-2a4445c1df73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f9o2lWdnc-kE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NTy6ULjhdXtV"
      },
      "outputs": [],
      "source": [
        "# Data loading and preprocessing\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, directory, transform=None):\n",
        "        self.directory = directory\n",
        "        self.transform = transform\n",
        "        self.classes, self.class_to_idx = self._find_classes(self.directory)\n",
        "        self.samples = self.make_dataset()\n",
        "\n",
        "    def _find_classes(self, dir):\n",
        "        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
        "        classes.sort()\n",
        "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "        return classes, class_to_idx\n",
        "\n",
        "    def make_dataset(self):\n",
        "        instances = []\n",
        "        for target_class in sorted(self.class_to_idx.keys()):\n",
        "            class_index = self.class_to_idx[target_class]\n",
        "            target_dir = os.path.join(self.directory, target_class)\n",
        "            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
        "                for fname in sorted(fnames):\n",
        "                    path = os.path.join(root, fname)\n",
        "                    if path.endswith('.jpg'):\n",
        "                        item = path, class_index\n",
        "                        instances.append(item)\n",
        "        return instances\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, target = self.samples[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "ulGfQfUWdlHv"
      },
      "outputs": [],
      "source": [
        "# Data augmentation and preprocessing\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Splitting dataset\n",
        "def create_data_loaders(directory, batch_size=64, num_workers=8, pin_memory=True):\n",
        "    dataset = CustomDataset(directory, transform=None)\n",
        "    train_idx, test_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42)\n",
        "\n",
        "    train_sampler = WeightedRandomSampler([1]*len(train_idx), len(train_idx))\n",
        "    test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
        "\n",
        "    train_dataset = CustomDataset(directory, transform=data_transforms['train'])\n",
        "    test_dataset = CustomDataset(directory, transform=data_transforms['test'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "                              num_workers=num_workers, pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler,\n",
        "                             num_workers=num_workers, pin_memory=pin_memory)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FOJZbeIidqPi"
      },
      "outputs": [],
      "source": [
        "# Model definition\n",
        "class ModifiedResNet50(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ModifiedResNet50, self).__init__()\n",
        "        self.base_model = models.resnet50(pretrained=True)\n",
        "        self.base_model.fc = nn.Sequential(\n",
        "            nn.Linear(self.base_model.fc.in_features, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base_model(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G_Iq0tLSdyKS"
      },
      "outputs": [],
      "source": [
        "# Training and evaluation\n",
        "def train_and_evaluate(train_loader, test_loader, device, num_epochs=10, learning_rate=0.001):\n",
        "    model = ModifiedResNet50(num_classes=50).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                print(f'Epoch {epoch+1}, Step {i}, Loss: {loss.item()}')\n",
        "\n",
        "    total_training_time = time.time() - start_time\n",
        "    print(f'Total training time: {total_training_time:.2f} seconds')\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy on the test set: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTTGC6RBd02Z",
        "outputId": "b8ee15fc-2160-41b8-e546-4ab6cb32d487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Step 0, Loss: 4.004199504852295\n",
            "Epoch 1, Step 20, Loss: 2.23380708694458\n",
            "Epoch 1, Step 40, Loss: 2.064950704574585\n",
            "Epoch 1, Step 60, Loss: 1.6351743936538696\n",
            "Epoch 1, Step 80, Loss: 1.4955179691314697\n",
            "Epoch 1, Step 100, Loss: 1.6246459484100342\n",
            "Epoch 1, Step 120, Loss: 1.636879801750183\n",
            "Epoch 2, Step 0, Loss: 1.0605998039245605\n",
            "Epoch 2, Step 20, Loss: 0.9838219881057739\n",
            "Epoch 2, Step 40, Loss: 1.2847704887390137\n",
            "Epoch 2, Step 60, Loss: 1.2749271392822266\n",
            "Epoch 2, Step 80, Loss: 1.4498692750930786\n",
            "Epoch 2, Step 100, Loss: 1.081357479095459\n",
            "Epoch 2, Step 120, Loss: 1.0814461708068848\n",
            "Epoch 3, Step 0, Loss: 1.1199069023132324\n",
            "Epoch 3, Step 20, Loss: 0.8310339450836182\n",
            "Epoch 3, Step 40, Loss: 0.9811151623725891\n",
            "Epoch 3, Step 60, Loss: 0.9070419073104858\n",
            "Epoch 3, Step 80, Loss: 0.9649494290351868\n",
            "Epoch 3, Step 100, Loss: 0.7771704196929932\n",
            "Epoch 3, Step 120, Loss: 0.8884932994842529\n",
            "Epoch 4, Step 0, Loss: 0.8512610793113708\n",
            "Epoch 4, Step 20, Loss: 0.8313724994659424\n",
            "Epoch 4, Step 40, Loss: 0.8041149377822876\n",
            "Epoch 4, Step 60, Loss: 1.0113765001296997\n",
            "Epoch 4, Step 80, Loss: 0.8065913915634155\n",
            "Epoch 4, Step 100, Loss: 0.9242966771125793\n",
            "Epoch 4, Step 120, Loss: 0.6855984330177307\n",
            "Epoch 5, Step 0, Loss: 0.6633552312850952\n",
            "Epoch 5, Step 20, Loss: 0.752061128616333\n",
            "Epoch 5, Step 40, Loss: 0.9484359622001648\n",
            "Epoch 5, Step 60, Loss: 0.7237736582756042\n",
            "Epoch 5, Step 80, Loss: 0.35713428258895874\n",
            "Epoch 5, Step 100, Loss: 0.9462727308273315\n",
            "Epoch 5, Step 120, Loss: 0.5482616424560547\n",
            "Epoch 6, Step 0, Loss: 0.5292090773582458\n",
            "Epoch 6, Step 20, Loss: 0.5482674241065979\n",
            "Epoch 6, Step 40, Loss: 0.6525201201438904\n",
            "Epoch 6, Step 60, Loss: 0.7123321890830994\n",
            "Epoch 6, Step 80, Loss: 0.6062943935394287\n",
            "Epoch 6, Step 100, Loss: 0.5955362915992737\n",
            "Epoch 6, Step 120, Loss: 0.547973096370697\n",
            "Epoch 7, Step 0, Loss: 0.5045488476753235\n",
            "Epoch 7, Step 20, Loss: 0.6126443147659302\n",
            "Epoch 7, Step 40, Loss: 0.4462939500808716\n",
            "Epoch 7, Step 60, Loss: 0.46181854605674744\n",
            "Epoch 7, Step 80, Loss: 0.472719669342041\n",
            "Epoch 7, Step 100, Loss: 0.5382271409034729\n",
            "Epoch 7, Step 120, Loss: 0.4196646809577942\n",
            "Epoch 8, Step 0, Loss: 0.6258582472801208\n",
            "Epoch 8, Step 20, Loss: 0.36756858229637146\n",
            "Epoch 8, Step 40, Loss: 0.6398188471794128\n",
            "Epoch 8, Step 60, Loss: 0.38320392370224\n",
            "Epoch 8, Step 80, Loss: 0.46338871121406555\n",
            "Epoch 8, Step 100, Loss: 0.46760761737823486\n",
            "Epoch 8, Step 120, Loss: 0.6180651187896729\n",
            "Epoch 9, Step 0, Loss: 0.5123538970947266\n",
            "Epoch 9, Step 20, Loss: 0.46949756145477295\n",
            "Epoch 9, Step 40, Loss: 0.4193437993526459\n",
            "Epoch 9, Step 60, Loss: 0.48856744170188904\n",
            "Epoch 9, Step 80, Loss: 0.5503098368644714\n",
            "Epoch 9, Step 100, Loss: 0.3689000904560089\n",
            "Epoch 9, Step 120, Loss: 0.43633797764778137\n",
            "Epoch 10, Step 0, Loss: 0.3617681860923767\n",
            "Epoch 10, Step 20, Loss: 0.5012629628181458\n",
            "Epoch 10, Step 40, Loss: 0.6729928255081177\n",
            "Epoch 10, Step 60, Loss: 0.3932987153530121\n",
            "Epoch 10, Step 80, Loss: 0.5037746429443359\n",
            "Epoch 10, Step 100, Loss: 0.5691741704940796\n",
            "Epoch 10, Step 120, Loss: 0.26964589953422546\n",
            "Total training time: 997.42 seconds\n",
            "Accuracy on the test set: 68.19%\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_loader, test_loader = create_data_loaders('/content/drive/MyDrive/JPEGImages')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_and_evaluate(train_loader, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
