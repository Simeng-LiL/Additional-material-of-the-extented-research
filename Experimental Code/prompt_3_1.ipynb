{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUJviF9n97ko",
        "outputId": "766fad3f-e841-4cd2-fd15-03680a3cdc43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "22jMdV5L-Axe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SAkWzbhG-CPO"
      },
      "outputs": [],
      "source": [
        "# Data loading and preprocessing\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load all images from the 'JPEGImages' folder\n",
        "dataset = ImageFolder(root='/content/drive/MyDrive/JPEGImages', transform=data_transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JptR0js4_kUB"
      },
      "outputs": [],
      "source": [
        "# Calculate weights for each class, adding a small constant 'eps' to avoid division by zero\n",
        "eps = 1e-6  # Small constant to prevent division by zero\n",
        "class_counts = np.array([len(np.where(np.array(dataset.targets) == i)[0]) for i in range(len(dataset.classes))])\n",
        "class_weights = 1. / (class_counts + eps)  # Adding eps to avoid division by zero\n",
        "weights = class_weights[np.array(dataset.targets)]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create WeightedRandomSampler for training set\n",
        "train_targets = [dataset.targets[i] for i in train_dataset.indices]\n",
        "train_samples_weight = torch.tensor([weights[i] for i in train_dataset.indices])\n",
        "train_sampler = WeightedRandomSampler(weights=train_samples_weight, num_samples=len(train_samples_weight), replacement=True)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler, num_workers=8, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkPjJ8rA9698",
        "outputId": "ba66171c-4882-4ef1-bb82-df32ada56167"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 210MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Model definition\n",
        "model = models.resnet50(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 50),  # 50 classes\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Enable mixed precision training\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CuZIQt-Q_vG_"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "def train_model():\n",
        "    model.train()\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        start_time = time.time()  # Start time of the epoch\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 20 == 19:  # print every 20 mini-batches\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 20:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Calculate and print the epoch duration\n",
        "        end_time = time.time()  # End time of the epoch\n",
        "        epoch_duration = end_time - start_time\n",
        "        print(f'Epoch {epoch + 1} completed in {epoch_duration:.2f} seconds')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ybl4gSM7_xR5"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "def evaluate_model():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy of the model on test images: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFPUkXUD_0QL",
        "outputId": "c48d8513-6c1b-4f5a-89b8-fa5d04d7c405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 20, Loss: 3.1989\n",
            "Epoch 1, Batch 40, Loss: 1.7519\n",
            "Epoch 1, Batch 60, Loss: 1.1045\n",
            "Epoch 1, Batch 80, Loss: 0.8974\n",
            "Epoch 1, Batch 100, Loss: 0.7296\n",
            "Epoch 1, Batch 120, Loss: 0.6488\n",
            "Epoch 1 completed in 429.38 seconds\n",
            "Epoch 2, Batch 20, Loss: 0.6312\n",
            "Epoch 2, Batch 40, Loss: 0.5240\n",
            "Epoch 2, Batch 60, Loss: 0.5417\n",
            "Epoch 2, Batch 80, Loss: 0.4854\n",
            "Epoch 2, Batch 100, Loss: 0.4578\n",
            "Epoch 2, Batch 120, Loss: 0.4190\n",
            "Epoch 2 completed in 172.25 seconds\n",
            "Epoch 3, Batch 20, Loss: 0.4178\n",
            "Epoch 3, Batch 40, Loss: 0.4066\n",
            "Epoch 3, Batch 60, Loss: 0.4214\n",
            "Epoch 3, Batch 80, Loss: 0.3921\n",
            "Epoch 3, Batch 100, Loss: 0.3573\n",
            "Epoch 3, Batch 120, Loss: 0.3734\n",
            "Epoch 3 completed in 81.97 seconds\n",
            "Epoch 4, Batch 20, Loss: 0.3286\n",
            "Epoch 4, Batch 40, Loss: 0.3945\n",
            "Epoch 4, Batch 60, Loss: 0.3355\n",
            "Epoch 4, Batch 80, Loss: 0.3424\n",
            "Epoch 4, Batch 100, Loss: 0.3653\n",
            "Epoch 4, Batch 120, Loss: 0.3490\n",
            "Epoch 4 completed in 58.87 seconds\n",
            "Epoch 5, Batch 20, Loss: 0.3014\n",
            "Epoch 5, Batch 40, Loss: 0.3183\n",
            "Epoch 5, Batch 60, Loss: 0.2950\n",
            "Epoch 5, Batch 80, Loss: 0.3261\n",
            "Epoch 5, Batch 100, Loss: 0.3017\n",
            "Epoch 5, Batch 120, Loss: 0.3139\n",
            "Epoch 5 completed in 39.91 seconds\n",
            "Epoch 6, Batch 20, Loss: 0.3135\n",
            "Epoch 6, Batch 40, Loss: 0.2925\n",
            "Epoch 6, Batch 60, Loss: 0.2722\n",
            "Epoch 6, Batch 80, Loss: 0.2887\n",
            "Epoch 6, Batch 100, Loss: 0.2932\n",
            "Epoch 6, Batch 120, Loss: 0.2982\n",
            "Epoch 6 completed in 32.21 seconds\n",
            "Epoch 7, Batch 20, Loss: 0.2963\n",
            "Epoch 7, Batch 40, Loss: 0.2719\n",
            "Epoch 7, Batch 60, Loss: 0.2592\n",
            "Epoch 7, Batch 80, Loss: 0.2609\n",
            "Epoch 7, Batch 100, Loss: 0.2429\n",
            "Epoch 7, Batch 120, Loss: 0.2808\n",
            "Epoch 7 completed in 30.29 seconds\n",
            "Epoch 8, Batch 20, Loss: 0.2853\n",
            "Epoch 8, Batch 40, Loss: 0.2957\n",
            "Epoch 8, Batch 60, Loss: 0.2482\n",
            "Epoch 8, Batch 80, Loss: 0.2508\n",
            "Epoch 8, Batch 100, Loss: 0.2406\n",
            "Epoch 8, Batch 120, Loss: 0.2673\n",
            "Epoch 8 completed in 28.65 seconds\n",
            "Epoch 9, Batch 20, Loss: 0.2284\n",
            "Epoch 9, Batch 40, Loss: 0.2731\n",
            "Epoch 9, Batch 60, Loss: 0.2620\n",
            "Epoch 9, Batch 80, Loss: 0.2474\n",
            "Epoch 9, Batch 100, Loss: 0.2097\n",
            "Epoch 9, Batch 120, Loss: 0.2359\n",
            "Epoch 9 completed in 27.57 seconds\n",
            "Epoch 10, Batch 20, Loss: 0.2056\n",
            "Epoch 10, Batch 40, Loss: 0.2410\n",
            "Epoch 10, Batch 60, Loss: 0.2254\n",
            "Epoch 10, Batch 80, Loss: 0.2035\n",
            "Epoch 10, Batch 100, Loss: 0.2157\n",
            "Epoch 10, Batch 120, Loss: 0.1996\n",
            "Epoch 10 completed in 27.97 seconds\n",
            "Accuracy of the model on test images: 89.61%\n"
          ]
        }
      ],
      "source": [
        "# Run training and evaluation\n",
        "train_model()\n",
        "evaluate_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
