{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dqAXNHw4m2X",
        "outputId": "a4d3b6c8-6fa2-48f9-8392-8cb40ae4b48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "tv2GNRNk4NjR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "icakLSqPZ26c"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data loading and preprocessing\n",
        "def load_data():\n",
        "    data_dir = '/content/drive/MyDrive/JPEGImages'\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load dataset without applying any transforms\n",
        "    full_dataset = ImageFolder(data_dir)\n",
        "\n",
        "    # Splitting dataset into train and test\n",
        "    train_indices, test_indices = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42)\n",
        "\n",
        "    # Apply different transforms for train and test datasets\n",
        "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "    test_dataset = torch.utils.data.Subset(full_dataset, test_indices)\n",
        "    train_dataset.dataset.transform = transform_train\n",
        "    test_dataset.dataset.transform = transform_test\n",
        "\n",
        "    # Calculate class weights for handling imbalance\n",
        "    class_counts = torch.bincount(torch.tensor(full_dataset.targets), minlength=50)\n",
        "    class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
        "    sample_weights = class_weights[full_dataset.targets]\n",
        "    train_sample_weights = sample_weights[train_indices]\n",
        "\n",
        "    # Create samplers\n",
        "    train_sampler = WeightedRandomSampler(train_sample_weights, len(train_indices), replacement=True)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, sampler=train_sampler, num_workers=8, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "UTl1T4_cj6-9"
      },
      "outputs": [],
      "source": [
        "# Model definition\n",
        "def initialize_model(num_classes=50):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(num_ftrs, num_classes),\n",
        "        nn.BatchNorm1d(num_classes)\n",
        "    )\n",
        "    model.to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "sxqtjBCIj9wp"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        epoch_duration = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1} completed in {epoch_duration:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "6VXnWteMdqnl"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYZgZ8DrdQMQ",
        "outputId": "549c0fe6-3e28-4921-d2c0-ce6dbf36d627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [1/129], Loss: 4.2647\n",
            "Epoch [1/10], Step [21/129], Loss: 2.1551\n",
            "Epoch [1/10], Step [41/129], Loss: 1.9102\n",
            "Epoch [1/10], Step [61/129], Loss: 1.8914\n",
            "Epoch [1/10], Step [81/129], Loss: 1.6256\n",
            "Epoch [1/10], Step [101/129], Loss: 1.6691\n",
            "Epoch [1/10], Step [121/129], Loss: 1.4305\n",
            "Epoch 1 completed in 29.42 seconds\n",
            "Epoch [2/10], Step [1/129], Loss: 1.5404\n",
            "Epoch [2/10], Step [21/129], Loss: 1.4630\n",
            "Epoch [2/10], Step [41/129], Loss: 1.3036\n",
            "Epoch [2/10], Step [61/129], Loss: 1.5324\n",
            "Epoch [2/10], Step [81/129], Loss: 1.2904\n",
            "Epoch [2/10], Step [101/129], Loss: 0.8860\n",
            "Epoch [2/10], Step [121/129], Loss: 1.1138\n",
            "Epoch 2 completed in 28.69 seconds\n",
            "Epoch [3/10], Step [1/129], Loss: 1.0593\n",
            "Epoch [3/10], Step [21/129], Loss: 0.9275\n",
            "Epoch [3/10], Step [41/129], Loss: 1.0424\n",
            "Epoch [3/10], Step [61/129], Loss: 1.2023\n",
            "Epoch [3/10], Step [81/129], Loss: 0.9401\n",
            "Epoch [3/10], Step [101/129], Loss: 0.7538\n",
            "Epoch [3/10], Step [121/129], Loss: 0.9458\n",
            "Epoch 3 completed in 28.80 seconds\n",
            "Epoch [4/10], Step [1/129], Loss: 0.9573\n",
            "Epoch [4/10], Step [21/129], Loss: 0.9469\n",
            "Epoch [4/10], Step [41/129], Loss: 1.0133\n",
            "Epoch [4/10], Step [61/129], Loss: 0.9272\n",
            "Epoch [4/10], Step [81/129], Loss: 0.7649\n",
            "Epoch [4/10], Step [101/129], Loss: 0.9063\n",
            "Epoch [4/10], Step [121/129], Loss: 0.9001\n",
            "Epoch 4 completed in 28.95 seconds\n",
            "Epoch [5/10], Step [1/129], Loss: 0.7944\n",
            "Epoch [5/10], Step [21/129], Loss: 0.7715\n",
            "Epoch [5/10], Step [41/129], Loss: 0.7721\n",
            "Epoch [5/10], Step [61/129], Loss: 0.7190\n",
            "Epoch [5/10], Step [81/129], Loss: 0.6662\n",
            "Epoch [5/10], Step [101/129], Loss: 0.8182\n",
            "Epoch [5/10], Step [121/129], Loss: 0.6907\n",
            "Epoch 5 completed in 28.82 seconds\n",
            "Epoch [6/10], Step [1/129], Loss: 0.5107\n",
            "Epoch [6/10], Step [21/129], Loss: 0.6430\n",
            "Epoch [6/10], Step [41/129], Loss: 0.7775\n",
            "Epoch [6/10], Step [61/129], Loss: 0.7987\n",
            "Epoch [6/10], Step [81/129], Loss: 0.5582\n",
            "Epoch [6/10], Step [101/129], Loss: 0.7708\n",
            "Epoch [6/10], Step [121/129], Loss: 0.7501\n",
            "Epoch 6 completed in 28.68 seconds\n",
            "Epoch [7/10], Step [1/129], Loss: 0.5229\n",
            "Epoch [7/10], Step [21/129], Loss: 0.4504\n",
            "Epoch [7/10], Step [41/129], Loss: 0.4377\n",
            "Epoch [7/10], Step [61/129], Loss: 0.6569\n",
            "Epoch [7/10], Step [81/129], Loss: 0.6801\n",
            "Epoch [7/10], Step [101/129], Loss: 0.5899\n",
            "Epoch [7/10], Step [121/129], Loss: 0.3992\n",
            "Epoch 7 completed in 28.84 seconds\n",
            "Epoch [8/10], Step [1/129], Loss: 0.4437\n",
            "Epoch [8/10], Step [21/129], Loss: 0.8101\n",
            "Epoch [8/10], Step [41/129], Loss: 0.3750\n",
            "Epoch [8/10], Step [61/129], Loss: 0.4912\n",
            "Epoch [8/10], Step [81/129], Loss: 0.4733\n",
            "Epoch [8/10], Step [101/129], Loss: 0.3980\n",
            "Epoch [8/10], Step [121/129], Loss: 0.2860\n",
            "Epoch 8 completed in 28.53 seconds\n",
            "Epoch [9/10], Step [1/129], Loss: 0.5309\n",
            "Epoch [9/10], Step [21/129], Loss: 0.5062\n",
            "Epoch [9/10], Step [41/129], Loss: 0.3749\n",
            "Epoch [9/10], Step [61/129], Loss: 0.5291\n",
            "Epoch [9/10], Step [81/129], Loss: 0.5835\n",
            "Epoch [9/10], Step [101/129], Loss: 0.2380\n",
            "Epoch [9/10], Step [121/129], Loss: 0.3935\n",
            "Epoch 9 completed in 28.80 seconds\n",
            "Epoch [10/10], Step [1/129], Loss: 0.5160\n",
            "Epoch [10/10], Step [21/129], Loss: 0.3150\n",
            "Epoch [10/10], Step [41/129], Loss: 0.3666\n",
            "Epoch [10/10], Step [61/129], Loss: 0.3475\n",
            "Epoch [10/10], Step [81/129], Loss: 0.5282\n",
            "Epoch [10/10], Step [101/129], Loss: 0.3563\n",
            "Epoch [10/10], Step [121/129], Loss: 0.4404\n",
            "Epoch 10 completed in 28.74 seconds\n",
            "Accuracy on test set: 63.14%\n"
          ]
        }
      ],
      "source": [
        "# Main\n",
        "def main():\n",
        "    train_loader, test_loader = load_data()\n",
        "    model = initialize_model()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    train_model(model, train_loader, criterion, optimizer)\n",
        "    evaluate_model(model, test_loader)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
