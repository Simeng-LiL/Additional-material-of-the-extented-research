{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dqAXNHw4m2X",
        "outputId": "a4d3b6c8-6fa2-48f9-8392-8cb40ae4b48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tv2GNRNk4NjR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet50\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xLpwf9r84rK6"
      },
      "outputs": [],
      "source": [
        "# Data Loading and Preprocessing with class weighting\n",
        "def load_data(data_dir, batch_size, num_workers, pin_memory):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
        "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
        "    ])\n",
        "\n",
        "    dataset = ImageFolder(root=data_dir, transform=transform)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Compute class weights\n",
        "    class_counts = np.zeros(len(dataset.classes))\n",
        "    for _, index in train_dataset:\n",
        "        class_counts[index] += 1\n",
        "    class_weights = 1.0 / (class_counts + 1e-6)  # Adding a small constant to avoid division by zero\n",
        "    sample_weights = [class_weights[label] for _, label in train_dataset]\n",
        "\n",
        "    # Setup the WeightedRandomSampler\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n",
        "    return train_loader, test_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wMKz52grXHbw"
      },
      "outputs": [],
      "source": [
        "# Model Definition\n",
        "def define_model(num_classes):\n",
        "    model = resnet50(pretrained=True)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(num_ftrs, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(512, num_classes),\n",
        "        nn.LogSoftmax(dim=1)\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "p-7fzvJhNGsF"
      },
      "outputs": [],
      "source": [
        "# Training the model with timing\n",
        "def train_model(model, train_loader, criterion, optimizer, device, epochs):\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()  # Start time for the epoch\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f'Epoch {epoch + 1}, Step {i + 1}, Loss: {running_loss / 20:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        end_time = time.time()  # End time for the epoch\n",
        "        epoch_duration = end_time - start_time  # Calculate duration of the epoch\n",
        "        print(f'Epoch {epoch + 1} completed in {epoch_duration:.2f} seconds')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UcaL-E7qW9Sv"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on the test set: {accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLeiN9JpW5-x",
        "outputId": "47a53070-88fc-4935-c900-a2286778e0b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Step 20, Loss: 3.5018\n",
            "Epoch 1, Step 40, Loss: 2.9120\n",
            "Epoch 1, Step 60, Loss: 2.7379\n",
            "Epoch 1, Step 80, Loss: 2.4131\n",
            "Epoch 1, Step 100, Loss: 2.3685\n",
            "Epoch 1, Step 120, Loss: 2.3184\n",
            "Epoch 1 completed in 28.57 seconds\n",
            "Epoch 2, Step 20, Loss: 2.2267\n",
            "Epoch 2, Step 40, Loss: 2.1365\n",
            "Epoch 2, Step 60, Loss: 1.8330\n",
            "Epoch 2, Step 80, Loss: 1.9202\n",
            "Epoch 2, Step 100, Loss: 1.7747\n",
            "Epoch 2, Step 120, Loss: 1.6653\n",
            "Epoch 2 completed in 28.34 seconds\n",
            "Epoch 3, Step 20, Loss: 1.6979\n",
            "Epoch 3, Step 40, Loss: 1.5737\n",
            "Epoch 3, Step 60, Loss: 1.4730\n",
            "Epoch 3, Step 80, Loss: 1.4452\n",
            "Epoch 3, Step 100, Loss: 1.4008\n",
            "Epoch 3, Step 120, Loss: 1.3102\n",
            "Epoch 3 completed in 28.42 seconds\n",
            "Epoch 4, Step 20, Loss: 1.3526\n",
            "Epoch 4, Step 40, Loss: 1.2426\n",
            "Epoch 4, Step 60, Loss: 1.1618\n",
            "Epoch 4, Step 80, Loss: 1.2531\n",
            "Epoch 4, Step 100, Loss: 1.0643\n",
            "Epoch 4, Step 120, Loss: 1.0931\n",
            "Epoch 4 completed in 28.40 seconds\n",
            "Epoch 5, Step 20, Loss: 1.1722\n",
            "Epoch 5, Step 40, Loss: 1.0224\n",
            "Epoch 5, Step 60, Loss: 0.9775\n",
            "Epoch 5, Step 80, Loss: 1.1043\n",
            "Epoch 5, Step 100, Loss: 1.0324\n",
            "Epoch 5, Step 120, Loss: 0.8432\n",
            "Epoch 5 completed in 28.57 seconds\n",
            "Epoch 6, Step 20, Loss: 0.9491\n",
            "Epoch 6, Step 40, Loss: 0.9293\n",
            "Epoch 6, Step 60, Loss: 0.7798\n",
            "Epoch 6, Step 80, Loss: 0.7673\n",
            "Epoch 6, Step 100, Loss: 0.8534\n",
            "Epoch 6, Step 120, Loss: 0.7659\n",
            "Epoch 6 completed in 28.48 seconds\n",
            "Epoch 7, Step 20, Loss: 0.7760\n",
            "Epoch 7, Step 40, Loss: 0.7877\n",
            "Epoch 7, Step 60, Loss: 0.6780\n",
            "Epoch 7, Step 80, Loss: 0.6535\n",
            "Epoch 7, Step 100, Loss: 0.7115\n",
            "Epoch 7, Step 120, Loss: 0.6728\n",
            "Epoch 7 completed in 28.62 seconds\n",
            "Epoch 8, Step 20, Loss: 0.6426\n",
            "Epoch 8, Step 40, Loss: 0.5970\n",
            "Epoch 8, Step 60, Loss: 0.5816\n",
            "Epoch 8, Step 80, Loss: 0.4832\n",
            "Epoch 8, Step 100, Loss: 0.5152\n",
            "Epoch 8, Step 120, Loss: 0.5051\n",
            "Epoch 8 completed in 28.33 seconds\n",
            "Epoch 9, Step 20, Loss: 0.6047\n",
            "Epoch 9, Step 40, Loss: 0.5627\n",
            "Epoch 9, Step 60, Loss: 0.5424\n",
            "Epoch 9, Step 80, Loss: 0.4885\n",
            "Epoch 9, Step 100, Loss: 0.5679\n",
            "Epoch 9, Step 120, Loss: 0.5309\n",
            "Epoch 9 completed in 28.49 seconds\n",
            "Epoch 10, Step 20, Loss: 0.5471\n",
            "Epoch 10, Step 40, Loss: 0.4523\n",
            "Epoch 10, Step 60, Loss: 0.4250\n",
            "Epoch 10, Step 80, Loss: 0.3728\n",
            "Epoch 10, Step 100, Loss: 0.4434\n",
            "Epoch 10, Step 120, Loss: 0.5342\n",
            "Epoch 10 completed in 28.35 seconds\n",
            "Accuracy on the test set: 63.72%\n"
          ]
        }
      ],
      "source": [
        "# Main function\n",
        "def main():\n",
        "    data_dir = '/content/drive/MyDrive/JPEGImages'\n",
        "    batch_size = 64\n",
        "    num_workers = 8\n",
        "    pin_memory = True\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_loader, test_loader = load_data(data_dir, batch_size, num_workers, pin_memory)\n",
        "    model = define_model(50)  # Assuming 50 animal classes\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    train_model(model, train_loader, criterion, optimizer, device, 10)\n",
        "    evaluate_model(model, test_loader, device)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
